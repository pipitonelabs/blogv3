---
title: Deploying My Astro Blog in a Self-Hosted Kubernetes HomeLab
description: 'A comprehensive guide to deploying an Astro blog in a self-hosted Kubernetes HomeLab using GitHub Actions and Renovate.'
publishDate: 2025-11-29
updatedDate: 2025-12-01
tags:
  - Kubernetes
  - Astro
  - GitHub Actions
  - Renovate
  - Flux
  - DevOps
  - Homelab
heroImage: { src: './k8s.png', color: '#1567c5ff' }
draft: true
enableComment: true
---

Running a personal blog in a self-hosted Kubernetes cluster has been an exciting journey. It combines the power of modern DevOps practices with the control of managing your own infrastructure. In this post, I'll walk you through how I set up continuous deployment for my Astro-based blog, from code commits to public access via `blog.pipitonelabs.com`. At the moment, this is an experiment to learn how to containerize this application with Node, as the production site is still running on Vercel at `pipitonelabs.com`.

GitHub Actions builds container images which my homelab Kubernetes cluster then pulls, deploys, updates, and exposes securely. Here's the high-level flow:

1. **Code Changes**: Push to the main branch triggers automated builds and releases.
2. **Dependency Updates**: Renovate detects package updates and creates pull requests.
3. **Deployment**: Flux (via HelmRelease) deploys new images automatically.
4. **Networking**: External-DNS creates DNS records, Envoy Gateway exposes the service, and Cloudflare Tunnel secures external access.

Let's dive into each component.

## Continuous Integration with GitHub Actions

The heart of my CI/CD pipeline is the `build-k8s.yaml` GitHub Actions workflow. It uses semantic versioning to create releases based on commit messages.

### Key Features:
- **Semantic Release**: Analyzes commit messages (e.g., `feat:`, `fix:`) to determine version bumps. Configured in [.releaserc.json](https://github.com/pipitonelabs/pipitonelabs.com/blob/main/.releaserc.json).
- **Docker Build**: Creates multi-platform images and pushes to GitHub Container Registry (GHCR) using the multi-stage [Dockerfile](https://github.com/pipitonelabs/pipitonelabs.com/blob/main/Dockerfile).
- **Tagging Strategy**: Generates version tags like `v1.2.3`, `v1.2`, `v1`, and SHA-based tags for flexibility.

### Semantic Release in Action

Semantic release analyzes conventional commit messages per [.releaserc.json](https://github.com/pipitonelabs/pipitonelabs.com/blob/main/.releaserc.json):

| Commit | Example | Bump | Tags |
|--------|---------|------|------|
| feat | `feat: add dark mode` | minor | v1.2.0, v1.2, v1 |
| fix/perf/revert/refactor | `fix: resolve login bug` | patch | v1.1.1, v1.1, v1 |
| docs/style/chore/etc | `docs: update readme` | none | SHA only |

The workflow runs on pushes to `main` or can be manually triggered. It outputs image metadata for easy consumption by deployment tools.

```yaml title=".github/workflows/build-k8s.yaml"
jobs:
  release:
    name: Semantic Release & Build Image
    runs-on: ubuntu-latest
    outputs:
      new_release_published: ${{ steps.semantic.outputs.new_release_published }}
      new_release_version: ${{ steps.semantic.outputs.new_release_version }}
  build-and-push:
    name: Build and Push Docker Image
    needs: release
    if: needs.release.outputs.new_release_published == 'true'
```

This ensures only meaningful changes trigger new deployments, reducing unnecessary builds.

## Docker Build Details

The optimized multi-stage [Dockerfile](https://github.com/pipitonelabs/pipitonelabs.com/blob/main/Dockerfile) enables **dual-deployment** with intelligent layer caching for faster builds.

### Multi-Stage Build Architecture

```
Base Stage (System deps)
    ↓
Dependencies Stage (npm packages)  
    ↓
Build Stage (Source code + Astro build)
    ↓
Runtime Stage (Alpine with Sharp runtime)
```

**Build Flow**:
1. **Base**: Install Sharp dependencies (`python3`, `build-essential`, `libvips-dev`)
2. **Dependencies**: `bun install` with package files only
3. **Build**: Copy source code, switch adapter, run `bun run build`
4. **Runtime**: Alpine Node with Sharp runtime (`vips`)

### Critical Docker Commands for Sharp

These commands are essential for Sharp image processing:

```dockerfile title="Sharp System Dependencies"
# Base stage - build dependencies
RUN apt-get update && apt-get install -y \
    python3         # Required for Sharp compilation
    build-essential # C++ compiler for native modules
    libvips-dev     # Sharp's underlying image library
    && rm -rf /var/lib/apt/lists/*

# Configure Sharp environment
ENV SHARP_IGNORE_GLOBAL_LIBVIPS=1

# Runtime stage - runtime dependencies
RUN apk add --no-cache vips  # Alpine package for libvips
```

### Adapter Switch Deep Dive

```dockerfile title="Dockerfile"
# Insert Node adapter import at line 2
RUN sed -i '2a import node from "@astrojs/node";' astro.config.ts && \
    # Replace Vercel adapter with Node standalone
    sed -i 's/adapter: vercel(),/adapter: node({ mode: "standalone" }),/' astro.config.ts
```

### Adapter Switch Context

**Vercel Adapter (`adapter: vercel()`)**:
- Serverless functions on edge runtime
- Automatic scaling, cold starts OK for previews

**Node Standalone (`adapter: node({ mode: 'standalone' })`)**:
- Single Node.js executable
- Docker/k8s native, no cold starts, full control

**Why Switch?**
- Vercel: Development/preview deployments
- Node: Production Kubernetes
- Single codebase, runtime-specific optimization

This sed command ensures genuine portability while keeping the repository history perfectly aligned across all environments — no branch divergence, no extra commits.

**Effect**: Serverless functions → single Node executable for Kubernetes.

### avatar.png Static Import Fix

**Issue**: `import.meta.glob` failed in Node adapter:

```javascript title="index.astro"
// ❌ Runtime glob fails in Node
const images = import.meta.glob('/src/assets/*.{png}')
<Image src={images[path]()} />
```

**Fix**: Static import:

```javascript title="index.astro"
// ✅ Build-time resolution
import avatarImage from '@/assets/avatar.png'
<Image src={avatarImage} />
```

**Why**: Vite glob is adapter-sensitive at runtime. Static imports are universal.

This makes the same code work for Vercel preview + k8s production.

## Optimized Docker Layer Caching

After experiencing slow build times, I optimized the Dockerfile with intelligent layer caching strategies to minimize rebuild times during development.

### Layer Caching Strategy

The key principle is ordering Docker instructions from **least frequently changing** to **most frequently changing**:

1. **Base Stage (Rarest Changes)**: System dependencies and Sharp libraries
2. **Dependencies Stage (Package Files)**: npm/bun packages 
3. **Build Stage (Source Code)**: Application code and build process

```dockerfile title="Dockerfile - Optimized Multi-Stage Build"
# Base stage with system dependencies (cached layer)
FROM oven/bun:1 AS base

# Install Sharp system dependencies (cached across builds)
RUN apt-get update && apt-get install -y \
    python3 build-essential libvips-dev \
    && rm -rf /var/lib/apt/lists/*

# Configure Sharp to use bundled libvips
ENV SHARP_IGNORE_GLOBAL_LIBVIPS=1

# Dependencies stage (package files)
FROM base AS deps

# Copy package files first (leverages Docker layer caching)
COPY package*.json bun.lock ./
RUN bun install

# Build stage (source code)
FROM deps AS build
COPY . .
RUN bun run build
```

### Performance Impact

| Change Type | Before Optimization | After Optimization |
|-------------|-------------------|-------------------|
| **Source code changes** | Full rebuild (2-5 min) | Final stage only (~30 sec) |
| **Dependency updates** | Full rebuild (2-5 min) | Dependencies stage only (~1 min) |

## Sharp Image Processing: Critical Configuration

Sharp (the high-performance image processing library) requires specific system dependencies and configuration to work reliably in containerized environments.

### System Dependencies

Sharp needs **native libraries** for image processing:

```bash
# Build-time dependencies (for compiling native modules)
python3        # Required for node-gyp compilation
build-essential # C/C++ compiler toolchain  
libvips-dev    # Sharp's underlying image library

# Runtime dependencies (for executing Sharp)
vips           # Alpine package for libvips runtime
```

### Critical Sharp Commands

These commands are essential for Sharp to function properly:

```dockerfile title="Sharp Configuration Commands"
# 1. Install build dependencies in build stage
RUN apt-get update && apt-get install -y \
    python3 build-essential libvips-dev

# 2. Configure Sharp environment variable
ENV SHARP_IGNORE_GLOBAL_LIBVIPS=1

# 3. Install runtime dependencies in runtime stage  
RUN apk add --no-cache vips
```

### Why SHARP_IGNORE_GLOBAL_LIBVIPS=1?

This environment variable tells Sharp to **use its bundled version of libvips** instead of any system-installed version. This ensures:

- **Consistent behavior** across different environments
- **No conflicts** with system libvips versions
- **Faster builds** since Sharp doesn't recompile
- **Reliable image processing** regardless of host system

### Common Sharp Issues in Docker

| Issue | Cause | Solution |
|-------|-------|----------|
| `apk: not found` | Using Alpine commands on Debian base | Use `apt-get` for `oven/bun:1` |
| `Cannot find module` | Wrong entry path in CMD | Use `dist/server/entry.mjs` |
| Sharp build failures | Missing system dependencies | Install `python3 build-essential libvips-dev` |
| Runtime errors | Missing libvips runtime | Add `ENV SHARP_IGNORE_GLOBAL_LIBVIPS=1` |

### Testing Sharp in Docker

To verify Sharp works correctly:

```bash
# Build the image
docker build -t my-astro-app .

# Run with port mapping
docker run -p 4321:4321 my-astro-app

# Test image processing endpoint
curl -I http://localhost:4321
```

The Astro site uses Sharp automatically for optimized images through the `astro:assets` integration.

## Automated Dependency Management with Renovate

Keeping dependencies up-to-date is crucial for security and features. I use Renovate, configured via a GitHub Actions workflow [renovate.yaml](https://github.com/pipitonelabs/k8s-gitops/blob/main/.github/workflows/renovate.yaml), to automate this process.

### How It Works:
- **Scheduled Runs**: Executes hourly to check for updates.
- **PR Creation**: When new versions are detected, Renovate creates pull requests with changelogs.
- **Merging Triggers Deployment**: Once I merge a PR, the CI pipeline builds a new image, and Flux deploys it.

The workflow uses a GitHub App for authentication, ensuring secure access without personal tokens.

```yaml title=".github/workflows/renovate.yaml"
- name: Run Renovate
  uses: renovatebot/github-action@03026bd55840025343414baec5d9337c5f9c7ea7 # v44.0.4
  env:
    LOG_LEVEL: "${{ inputs.logLevel || 'debug' }}"
    RENOVATE_AUTODISCOVER: true
    RENOVATE_AUTODISCOVER_FILTER: "${{ github.repository }}"
```

This setup keeps my blog current with the latest Astro, Node.js, and other dependencies without manual intervention.

## Deployment with Helm and Flux

For deployment, I use Flux's HelmRelease powered by the [bjw-s-labs/app-template](https://github.com/bjw-s-labs/helm-charts) chart. This replaces writing full custom Helm charts for every app with concise `values.yaml` files (50-150 lines). It leverages the `bjw-s/common` library for best practices like security contexts, probes, Gateway API routes, and schema validation. A single chart upgrade propagates features and fixes across all my apps instantly, keeping my Git repo lean and drift-free.

The HelmRelease specifies the image tag (updated manually or via automation):

```yaml title="helmrelease.yaml"
containers:
  main:
    image:
      repository: ghcr.io/pipitonelabs/pipitonelabs.com
      tag: v1.0.7
```

Flux watches for new image tags and automatically updates the deployment. This GitOps approach ensures the cluster state matches the desired configuration.

## Networking and Exposure

Getting the blog accessible from the internet involves several layers:

### External-DNS, Envoy Gateway, and Cloudflare Tunnel

The HelmRelease configures the route directly:

```yaml title="helmrelease.yaml"
# Continued from HelmRelease above
  route:
    app:
      hostnames:
        - blog.pipitonelabs.com
      parentRefs:
        - name: envoy-external
          namespace: networking
```

External-DNS syncs the hostname to DNS, Envoy Gateway (via `envoy-external`) handles ingress routing, and Cloudflare Tunnel provides secure exposure without open ports.

### Envoy Gateway
Acts as the ingress controller built for the Gateway API, routing external traffic to internal services. It supports advanced features like TLS termination and load balancing.

### Cloudflare Tunnel
Provides secure, encrypted tunnels from my homelab to Cloudflare's edge. This avoids exposing ports directly on my home network, enhancing security.

Together, these tools create a seamless path from `https://blog.pipitonelabs.com` to my Kubernetes pods.

## Conclusion

This setup demonstrates the power of combining open-source tools for a robust, automated deployment pipeline. From local development to global access, every step is streamlined and secure.

The beauty of Kubernetes in a homelab is the learning opportunity—it mirrors production environments while giving full control. If you're considering a similar setup, start small: get a basic deployment working, then layer on automation.

---

*Have questions about the setup? Drop a comment below.*